---
title: "Prediction-Assignment"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This work was made for the project assignment of the Pratical Machine Learning Course hosted in the Coursera platform. Our goal was to construct an algorithm to predict, based on kinetic parameters given by a range of body sensors, if an individual performed a weight lifting exercise according specification (Class A output) or with different types of errors (Classes B, C, D and E). We had at our disposal two data sets, one to build the model and other to validate it. All the data comes from the Human Activity Recognition (HAR) group of the Rio de Janeiro PUC university (http://groupware.les.inf.puc-rio.br/har).

#Getting and reading the data sets

```{r }
# Download data
trainingUrl <- "pml-training.csv"
testingUrl <- "pml-testing.csv"

```

```{r }

# Read data
training <- read.csv(trainingUrl, na.strings = c("NA", "#DIV/0!"))
testing <- read.csv(testingUrl, na.strings = c("NA", "#DIV/0!"))

```
##Looking at the data
```{r }
library(dplyr)


```
##Cleaning data
```{r }
# Remove variables in the training set with too much NAs 
goodCol <- colSums(is.na(training)) < 1900
myTraining <- training[ , goodCol][ , ]

```

```{r }
# Remove the same columns in the test set
myTesting <- testing[ , goodCol][ , ]

```

```{r }
# Remove the first seven columns in both sets
myTraining <- myTraining[ , -(1:7)]
myTesting <- myTesting[ , -(1:7)] 


```
Now we have 19622 observations of 53 variables (training) and 20 observations of 53 variables (testing).

##Subsetting the training data

In building our model, for a cross validation objective, we subset our training data to a real training set and a test set.

```{r }
# Create inTraining and inTesting
library(caret)
set.seed(4848)
inTrain <- createDataPartition(y = myTraining$classe, p = 0.75, list = FALSE)
inTraining <- myTraining[inTrain, ]
inTesting <- myTraining[-inTrain, ]

```
##Building the model

Tree methods were tried: gradient boosting with "gbm", random forests with "rf" and random forests using the randomForest() functiom. The first two revealed themselves to be painfully slow, so they were disregarded and randomForest was choosed to training, tunning and testing.
```{r }
# Train with randomForest
library(randomForest)

```

```{r }
set.seed(555)
rfGrid <-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:30)*50,
                        shrinkage = 0.1)
modelFit <- randomForest(classe ~., data = inTraining, tuneGrid = rfGrid) 

plot(modelFit)

```
This model looked promissing, with very low classification errors in all classes, and a Out of the Bag (OOB) error estimate that descends swiftly to near 0, as we can see in the plot above.

##Cross validation
```{r }

# Test "out of sample"
predictions <- predict(modelFit, newdata = inTesting)
confusionMatrix(predictions, inTesting$classe)

```
##Final validation with results for submission
```{r }

# Test validation sample
answers <- predict(modelFit, newdata = myTesting, type = "response")
print(answers)
```

##Conclusion

All the 20 answers were validated as correct at the PML project submission page.
